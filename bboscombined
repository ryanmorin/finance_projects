import pandas as pd
import numpy as np
import math
import os
import config

PATH = 'XXXX'
LESSER_FILE = 'XXX'
GREATER_FILE = 'XXXX'
DEPRECIATION_FILE = 'XXXX'


def calculate_inventory_owned_flag(row) -> str:
    # Your calculate_inventory_owned_flag function remains unchanged
    # wholesaler_id = 263 --> Member Sourcing --> Rule a/p Rob
    if row['purchase_type'] == 'consignment' and row['wholesaler_id'] == 343:
        return 'no' # upteam consignment
    elif row['purchase_type'] == 'consignment' and (row['cost'] <= 1 or math.isnan(row['cost'])):
        return 'no' # other consigners ie. Kuwait
    elif row['purchase_type'] == 'private_sale_member_sourcing' and (row['cost'] <= 1 or math.isnan(row['cost'])):
        return 'no' # misc mis consigners
    else:
        return 'yes'


def depreciation_gaps(input_df):
    # Condition 1: Set values for rental, row=1, inventory_owned_flag='yes', total_depreciation=NULL
    condition1 = (input_df['inventory_classification'] == 'rental') & \
                 (input_df['row_number'] == 1) & \
                 (input_df['inventory_owned_flag'] == 'yes') & \
                 (input_df['total_depreciation'].isnull())

    input_df.loc[condition1, 'total_depreciation'] = input_df.loc[condition1, 'cost'] * 0.83
    input_df.loc[condition1, ['beginning_value', 'ending_value']] = input_df.loc[condition1, 'cost'] * 0.17
    input_df.loc[condition1, 'monthly_depreciation'] = 0

    # Calculate ro_max_cost_df for the entire input_df
    ro_max_cost_df = input_df.groupby('asset_id')['cost'].max().reset_index()

    # Condition 2: Update values where conditions are met
    condition2 = (input_df['inventory_classification'] == 'rental_to_outlet') & \
                 (input_df['row_number'] == 1) & \
                 (input_df['inventory_owned_flag'] == 'yes') & \
                 (input_df['total_depreciation'].isnull())

    # Merge input_df and ro_max_cost_df on asset_id for condition2
    input_df = input_df.merge(ro_max_cost_df, on='asset_id', suffixes=('', '_max_cost'), how='left')
    input_df.loc[condition2, 'total_depreciation'] = input_df.loc[condition2, 'cost_max_cost'] - input_df['cost']
    input_df.loc[condition2, ['beginning_value', 'ending_value']] = input_df['cost']
    input_df.loc[condition2, 'monthly_depreciation'] = 0

    return input_df
    

def clean_file(file_name: str, path: str) -> pd.DataFrame:
    # Your clean_file function remains unchanged
    # column names dictionary
    col_names_dict = {
        'as_of_date': 'as_of_date',
        'unique_inventoryid': 'unique_inventory_id',
        'asset_ID': 'asset_id',
        'productID': 'product_id',# delete
        'location': 'location',
        'Location Date': 'location_date',
        'Date Received': 'date_received',
        'Cost': 'cost',
        'product_type': 'product_type',# delete
        'brand': 'brand',# delete
        'purchase_type': 'purchase_type',
        'Site': 'site',
        'product_name': 'product_name',# delete
        'MSRP': 'msrp',# delete
        'Physical Location': 'physical_location',# delete
        'level': 'level', # delete
        'Availability': 'availability', # delete
        'wholesalerID': 'wholesaler_id',
        'Wholesaler': 'wholesaler'
    }

    # Use pd.read_csv() to read the CSV file into a Pandas DataFrame
    df = pd.read_csv(f'{path}{file_name}')

    # Rename the column names using col_names_dict
    df = df.rename(columns=col_names_dict)

    # clean some of the columns
    df['cost'] = df['cost'].replace('NULL', '0')
    df['cost'] = df['cost'].replace('', '0')
    df['purchase_type'] = df['purchase_type'].str.lower()
    df['site'] = df['site'].str.lower()
    df['location'] = df['location'].str.lower()

    # Convert the 'cost' column to a numeric type (in case it's not already)
    df['cost'] = pd.to_numeric(df['cost'], errors='coerce')

    # Select important fields from inventory
    important_fields = ['as_of_date',
                        'asset_id',
                        'cost',
                        'date_received',
                        'location',
                        'location_date',
                        'purchase_type',
                        'site',
                        'unique_inventory_id',
                        'wholesaler',
                        'wholesaler_id']

    # Create new dataframe with important fields
    new_df = df[important_fields]

    # Create new column called inventory_owned_flag --> boolean 'yes', 'no'
    new_df.loc[:, 'inventory_owned_flag'] = new_df.apply(calculate_inventory_owned_flag, axis=1)

    # Create new column called inventory_classification --> 'rental, outlet, consignment, outlet_to_rental, rental_to_outlet'
    conditions = [
        (new_df['inventory_owned_flag'] == 'yes') & (new_df['location'] == 'moved_to_rental'), # category that indicates the movement between buckets
        (new_df['inventory_owned_flag'] == 'yes') & (new_df['location'] == 'moved_to_outlet'), # category that indicates the movement between buckets
        (new_df['inventory_owned_flag'] == 'yes') & (new_df['purchase_type'] == 'consignment'), #
        (new_df['inventory_owned_flag'] == 'no') & (new_df['purchase_type'] == 'consignment') & (new_df['site'] == 'rental'),
        (new_df['inventory_owned_flag'] == 'no') & (new_df['purchase_type'] == 'consignment') & (new_df['site'] == 'outlet'),
        (new_df['purchase_type'] == 'rental') & (new_df['site'] == 'rental'),
        (new_df['purchase_type'] == 'rental') & (new_df['site'] == 'outlet'),
        (new_df['purchase_type'] != 'rental') & (new_df['purchase_type'] != 'consignment') & (new_df['site'] == 'outlet'),
        (new_df['purchase_type'] != 'rental') & (new_df['purchase_type'] != 'consignment') & (new_df['site'] == 'rental')
    ]

    # The conditions above will me mapped to the inventory tags below
    values = ['rental', 'rental_to_outlet', 'outlet', 'rental', 'consignment', 'rental', 'rental_to_outlet',
              'outlet', 'rental']

    # Use numpy.select to create the 'inventory_classification' column
    new_df.loc[:, 'inventory_classification'] = np.select(conditions, values, default=None)

    # first make sure that dates in date format
    new_df['location_date'] = pd.to_datetime(new_df['location_date'])

    # Sort the DataFrame by 'date' in descending order --> newest first
    new_df = new_df.sort_values(by='location_date', ascending=False)

    # create a row number - 1 is the newest - most recent location for inventory
    new_df['row_number'] = new_df.groupby('asset_id')['location_date'].rank(method='dense', ascending=False)

    # return the resulting transformed data frame
    return new_df


df_lesser_clean = clean_file(file_name=LESSER_FILE, path=PATH)
df_greater_clean = clean_file(file_name=GREATER_FILE, path=PATH)


def clean_and_merge_data(greater_df: pd.DataFrame, lesser_df: pd.DataFrame) -> pd.DataFrame:
    # make sure that the date fields are converted to datetime
    greater_df['as_of_date'] = pd.to_datetime(greater_df['as_of_date'])
    lesser_df['location_date'] = pd.to_datetime(lesser_df['location_date'])

    # mask that will be applied to determine the rows that should remain
    mask = (
            ((lesser_df['location'] == 'consignment_expired') & (greater_df['location'] == 'consignment_expired')) |
            ((lesser_df['location'] == 'customer_theft') & (greater_df['location'] == 'customer_theft')) |
            ((lesser_df['location'] == 'lost_at_warehouse') & (greater_df['location'] == 'lost_at_warehouse')) |
            ((lesser_df['location'] == 'lost_by_customer') & (greater_df['location'] == 'lost_by_customer')) |
            ((lesser_df['location'] == 'lost_by_ups') & (greater_df['location'] == 'lost_by_ups')) |
            ((lesser_df['location'] == 'permanently_grounded') & (
                    greater_df['location'] == 'permanently_grounded')) |
            ((lesser_df['location'] == 'promotional_giveaway') & (
                    greater_df['location'] == 'promotional_giveaway')) |
            ((lesser_df['location'] == 'purchased') & (greater_df['location'] == 'purchased')) |
            ((lesser_df['location'] == 'receiving_error_does_not_exist') & (greater_df['location'] ==
                                                                            'receiving_error_does_not_exist')) |
            ((lesser_df['location'] == 'recouped') & (greater_df['location'] == 'recouped')) |
            ((lesser_df['location'] == 'retired') & (greater_df['location'] == 'purchased')) |
            ((lesser_df['location'] == 'retired') & (greater_df['location'] == 'retired')) |
            ((lesser_df['location'] == 'returned_to_vendor') & (greater_df['location'] == 'returned_to_vendor')) |
            ((lesser_df['location'] == 'stolen') & (greater_df['location'] == 'stolen')) |
            ((lesser_df['location'] == 'moved_to_outlet') & (greater_df['location'] == 'moved_to_outlet')) |
            ((lesser_df['location'] == 'moved_to_rental') & (greater_df['location'] == 'moved_to_rental')) |
            ((lesser_df['location'].isnull()) & (greater_df['location'] == 'permanently_grounded')) |
            ((lesser_df['location'].isnull()) & (greater_df['location'] == 'purchased')) |
            ((lesser_df['location'].isnull()) & (greater_df['location'] == 'receiving_error_does_not_exist')) |
            ((lesser_df['location'].isnull()) & (greater_df['location'] == 'returned_to_vendor')) &
            ((greater_df['as_of_date'] - lesser_df['location_date']).dt.days > 6 * 30)
    )

    # Apply the mask to the DataFrames to delete rows
    greater_df = greater_df[~greater_df['unique_inventory_id'].isin(lesser_df.loc[mask, 'unique_inventory_id'])]
    lesser_df = lesser_df[~mask]

    return greater_df, lesser_df


# Delete the old inventory and merge the DataFrames
cleaned_greater_date_df, cleaned_lesser_date_df = clean_and_merge_data(df_greater_clean, df_lesser_clean)

# Add new columns to the resulting dataframes and add depreciation data to the greatest
dep_df = pd.read_csv(os.path.join(PATH, DEPRECIATION_FILE))
dep_df = dep_df[['asset_id', 'beginning_value', 'ending_value', 'monthly_depreciation', 'total_depreciation']]

# Perform a left join on 'asset_id'
cleaned_greater_date_df = cleaned_greater_date_df.merge(dep_df, on='asset_id', how='left')

# Fill in the missing depreciation values for fully depreciated items
cleaned_greater_date_df = depreciation_gaps(input_df=cleaned_greater_date_df)

# Define cleaned file names
cleaned_greater_date_file = GREATER_FILE.replace('.csv', '_clean.csv')
cleaned_lesser_date_file = LESSER_FILE.replace('.csv', '_clean.csv')

# Save final dataframes if needed
cleaned_greater_date_df.to_csv(cleaned_greater_date_file, index=False)
cleaned_lesser_date_df.to_csv(cleaned_lesser_date_file, index=False)
